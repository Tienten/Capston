queue/partition is batch
running on h3
work directory is /cluster/home/ttphan21/Capstone
train: return targets and labels, relabel run after 1 epoch and update on label from DCNN, predict get from resnet: lr=0.001, momentum = sgd  
/mounts/hamilton/software/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mounts/hamilton/software/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
data_2/FER2013/train
data_2/FER2013/train/neutral
data_2/FER2013/train/happy
data_2/FER2013/train/surprise
data_2/FER2013/train/fear
data_2/FER2013/train/disgust
data_2/FER2013/train/sad
data_2/FER2013/train/angry
Train set size: 7178
[Epoch 1] Accuracy: 0.4628. Loss: 1.486
[Epoch 2] Accuracy: 0.6034. Loss: 1.235
[Epoch 3] Accuracy: 0.6134. Loss: 1.194
[Epoch 4] Accuracy: 0.6261. Loss: 1.134
[Epoch 5] Accuracy: 0.6360. Loss: 1.086
[Epoch 6] Accuracy: 0.6500. Loss: 1.040
[Epoch 7] Accuracy: 0.6548. Loss: 0.983
[Epoch 8] Accuracy: 0.6744. Loss: 0.924
[Epoch 9] Accuracy: 0.6931. Loss: 0.864
[Epoch 10] Accuracy: 0.7168. Loss: 0.793
[Epoch 11] Accuracy: 0.7466. Loss: 0.709
[Epoch 12] Accuracy: 0.7864. Loss: 0.626
[Epoch 13] Accuracy: 0.8122. Loss: 0.542
[Epoch 14] Accuracy: 0.8387. Loss: 0.476
[Epoch 15] Accuracy: 0.8718. Loss: 0.381
[Epoch 16] Accuracy: 0.8950. Loss: 0.332
[Epoch 17] Accuracy: 0.9039. Loss: 0.292
[Epoch 18] Accuracy: 0.9121. Loss: 0.269
[Epoch 19] Accuracy: 0.9249. Loss: 0.238
[Epoch 20] Accuracy: 0.9340. Loss: 0.217
[Epoch 21] Accuracy: 0.9407. Loss: 0.198
[Epoch 22] Accuracy: 0.9426. Loss: 0.180
[Epoch 23] Accuracy: 0.9517. Loss: 0.164
[Epoch 24] Accuracy: 0.9521. Loss: 0.155
[Epoch 25] Accuracy: 0.9565. Loss: 0.146
[Epoch 26] Accuracy: 0.9570. Loss: 0.150
[Epoch 27] Accuracy: 0.9549. Loss: 0.148
[Epoch 28] Accuracy: 0.9618. Loss: 0.128
[Epoch 29] Accuracy: 0.9596. Loss: 0.137
[Epoch 30] Accuracy: 0.9607. Loss: 0.127
[Epoch 31] Accuracy: 0.9652. Loss: 0.117
[Epoch 32] Accuracy: 0.9673. Loss: 0.115
[Epoch 33] Accuracy: 0.9635. Loss: 0.122
[Epoch 34] Accuracy: 0.9645. Loss: 0.120
[Epoch 35] Accuracy: 0.9723. Loss: 0.100
[Epoch 36] Accuracy: 0.9685. Loss: 0.109
[Epoch 37] Accuracy: 0.9684. Loss: 0.108
[Epoch 38] Accuracy: 0.9699. Loss: 0.102
[Epoch 39] Accuracy: 0.9680. Loss: 0.106
[Epoch 40] Accuracy: 0.9705. Loss: 0.105
[Epoch 41] Accuracy: 0.9694. Loss: 0.101
[Epoch 42] Accuracy: 0.9739. Loss: 0.094
[Epoch 43] Accuracy: 0.9716. Loss: 0.093
[Epoch 44] Accuracy: 0.9685. Loss: 0.104
[Epoch 45] Accuracy: 0.9724. Loss: 0.096
[Epoch 46] Accuracy: 0.9699. Loss: 0.098
[Epoch 47] Accuracy: 0.9717. Loss: 0.098
[Epoch 48] Accuracy: 0.9741. Loss: 0.089
[Epoch 49] Accuracy: 0.9719. Loss: 0.097
[Epoch 50] Accuracy: 0.9703. Loss: 0.098
[Epoch 51] Accuracy: 0.9756. Loss: 0.087
[Epoch 52] Accuracy: 0.9734. Loss: 0.091
[Epoch 53] Accuracy: 0.9728. Loss: 0.093
[Epoch 54] Accuracy: 0.9727. Loss: 0.095
[Epoch 55] Accuracy: 0.9751. Loss: 0.090
[Epoch 56] Accuracy: 0.9727. Loss: 0.095
[Epoch 57] Accuracy: 0.9727. Loss: 0.093
[Epoch 58] Accuracy: 0.9730. Loss: 0.094
[Epoch 59] Accuracy: 0.9785. Loss: 0.081
[Epoch 60] Accuracy: 0.9758. Loss: 0.088
[Epoch 61] Accuracy: 0.9723. Loss: 0.092
[Epoch 62] Accuracy: 0.9731. Loss: 0.093
[Epoch 63] Accuracy: 0.9741. Loss: 0.090
[Epoch 64] Accuracy: 0.9745. Loss: 0.089
[Epoch 65] Accuracy: 0.9703. Loss: 0.091
[Epoch 66] Accuracy: 0.9753. Loss: 0.087
[Epoch 67] Accuracy: 0.9688. Loss: 0.102
[Epoch 68] Accuracy: 0.9730. Loss: 0.090
[Epoch 69] Accuracy: 0.9726. Loss: 0.097
[Epoch 70] Accuracy: 0.9751. Loss: 0.089
Best Acc: 0.978546
Accuracy:  [0.4628, 0.6034, 0.6134, 0.6261, 0.636, 0.65, 0.6548, 0.6744, 0.6931, 0.7168, 0.7466, 0.7864, 0.8122, 0.8387, 0.8718, 0.895, 0.9039, 0.9121, 0.9249, 0.934, 0.9407, 0.9426, 0.9517, 0.9521, 0.9565, 0.957, 0.9549, 0.9618, 0.9596, 0.9607, 0.9652, 0.9673, 0.9635, 0.9645, 0.9723, 0.9685, 0.9684, 0.9699, 0.968, 0.9705, 0.9694, 0.9739, 0.9716, 0.9685, 0.9724, 0.9699, 0.9717, 0.9741, 0.9719, 0.9703, 0.9756, 0.9734, 0.9728, 0.9727, 0.9751, 0.9727, 0.9727, 0.973, 0.9785, 0.9758, 0.9723, 0.9731, 0.9741, 0.9745, 0.9703, 0.9753, 0.9688, 0.973, 0.9726, 0.9751]
