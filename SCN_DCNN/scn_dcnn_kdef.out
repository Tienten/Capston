queue/partition is batch
running on h0
work directory is /cluster/home/ttphan21/Capstone
train: return targets and labels, relabel run after 1 epoch and update on label from DCNN, predict get from resnet: lr=0.001, momentum = sgd  
/mounts/hamilton/software/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/mounts/hamilton/software/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
data_2/KDEF/train
data_2/KDEF/train/HA
data_2/KDEF/train/SU
data_2/KDEF/train/SA
data_2/KDEF/train/DI
data_2/KDEF/train/AF
data_2/KDEF/train/AN
data_2/KDEF/train/NE
Train set size: 490
[Epoch 1] Accuracy: 0.1653. Loss: 1.948
[Epoch 2] Accuracy: 0.2204. Loss: 1.889
[Epoch 3] Accuracy: 0.5143. Loss: 1.316
[Epoch 4] Accuracy: 0.8122. Loss: 0.739
[Epoch 5] Accuracy: 0.8490. Loss: 0.565
[Epoch 6] Accuracy: 0.9102. Loss: 0.468
[Epoch 7] Accuracy: 0.9122. Loss: 0.396
[Epoch 8] Accuracy: 0.9000. Loss: 0.381
[Epoch 9] Accuracy: 0.8959. Loss: 0.360
[Epoch 10] Accuracy: 0.8857. Loss: 0.431
[Epoch 11] Accuracy: 0.8980. Loss: 0.349
[Epoch 12] Accuracy: 0.9224. Loss: 0.296
[Epoch 13] Accuracy: 0.9286. Loss: 0.244
[Epoch 14] Accuracy: 0.9184. Loss: 0.265
[Epoch 15] Accuracy: 0.9204. Loss: 0.268
[Epoch 16] Accuracy: 0.9347. Loss: 0.244
[Epoch 17] Accuracy: 0.9327. Loss: 0.224
[Epoch 18] Accuracy: 0.9429. Loss: 0.216
[Epoch 19] Accuracy: 0.9306. Loss: 0.250
[Epoch 20] Accuracy: 0.9449. Loss: 0.213
[Epoch 21] Accuracy: 0.9449. Loss: 0.210
[Epoch 22] Accuracy: 0.9347. Loss: 0.201
[Epoch 23] Accuracy: 0.9490. Loss: 0.167
[Epoch 24] Accuracy: 0.9510. Loss: 0.181
[Epoch 25] Accuracy: 0.9531. Loss: 0.165
[Epoch 26] Accuracy: 0.9469. Loss: 0.174
[Epoch 27] Accuracy: 0.9776. Loss: 0.105
[Epoch 28] Accuracy: 0.9510. Loss: 0.154
[Epoch 29] Accuracy: 0.9633. Loss: 0.136
[Epoch 30] Accuracy: 0.9551. Loss: 0.190
[Epoch 31] Accuracy: 0.9469. Loss: 0.188
[Epoch 32] Accuracy: 0.9571. Loss: 0.168
[Epoch 33] Accuracy: 0.9429. Loss: 0.184
[Epoch 34] Accuracy: 0.9612. Loss: 0.145
[Epoch 35] Accuracy: 0.9633. Loss: 0.132
[Epoch 36] Accuracy: 0.9571. Loss: 0.157
[Epoch 37] Accuracy: 0.9633. Loss: 0.157
[Epoch 38] Accuracy: 0.9694. Loss: 0.122
[Epoch 39] Accuracy: 0.9673. Loss: 0.137
[Epoch 40] Accuracy: 0.9571. Loss: 0.169
[Epoch 41] Accuracy: 0.9694. Loss: 0.133
[Epoch 42] Accuracy: 0.9673. Loss: 0.144
[Epoch 43] Accuracy: 0.9714. Loss: 0.117
[Epoch 44] Accuracy: 0.9735. Loss: 0.120
[Epoch 45] Accuracy: 0.9694. Loss: 0.146
[Epoch 46] Accuracy: 0.9612. Loss: 0.142
[Epoch 47] Accuracy: 0.9653. Loss: 0.132
[Epoch 48] Accuracy: 0.9714. Loss: 0.105
[Epoch 49] Accuracy: 0.9612. Loss: 0.138
[Epoch 50] Accuracy: 0.9857. Loss: 0.094
[Epoch 51] Accuracy: 0.9714. Loss: 0.123
[Epoch 52] Accuracy: 0.9837. Loss: 0.087
[Epoch 53] Accuracy: 0.9673. Loss: 0.122
[Epoch 54] Accuracy: 0.9714. Loss: 0.117
[Epoch 55] Accuracy: 0.9633. Loss: 0.141
[Epoch 56] Accuracy: 0.9653. Loss: 0.145
[Epoch 57] Accuracy: 0.9531. Loss: 0.168
[Epoch 58] Accuracy: 0.9673. Loss: 0.142
[Epoch 59] Accuracy: 0.9653. Loss: 0.128
[Epoch 60] Accuracy: 0.9776. Loss: 0.107
[Epoch 61] Accuracy: 0.9612. Loss: 0.171
[Epoch 62] Accuracy: 0.9816. Loss: 0.090
[Epoch 63] Accuracy: 0.9388. Loss: 0.195
[Epoch 64] Accuracy: 0.9592. Loss: 0.125
[Epoch 65] Accuracy: 0.9592. Loss: 0.159
[Epoch 66] Accuracy: 0.9673. Loss: 0.139
[Epoch 67] Accuracy: 0.9449. Loss: 0.172
[Epoch 68] Accuracy: 0.9673. Loss: 0.117
[Epoch 69] Accuracy: 0.9571. Loss: 0.170
[Epoch 70] Accuracy: 0.9653. Loss: 0.145
Best Acc: 0.985714
Accuracy:  [0.1653, 0.2204, 0.5143, 0.8122, 0.849, 0.9102, 0.9122, 0.9, 0.8959, 0.8857, 0.898, 0.9224, 0.9286, 0.9184, 0.9204, 0.9347, 0.9327, 0.9429, 0.9306, 0.9449, 0.9449, 0.9347, 0.949, 0.951, 0.9531, 0.9469, 0.9776, 0.951, 0.9633, 0.9551, 0.9469, 0.9571, 0.9429, 0.9612, 0.9633, 0.9571, 0.9633, 0.9694, 0.9673, 0.9571, 0.9694, 0.9673, 0.9714, 0.9735, 0.9694, 0.9612, 0.9653, 0.9714, 0.9612, 0.9857, 0.9714, 0.9837, 0.9673, 0.9714, 0.9633, 0.9653, 0.9531, 0.9673, 0.9653, 0.9776, 0.9612, 0.9816, 0.9388, 0.9592, 0.9592, 0.9673, 0.9449, 0.9673, 0.9571, 0.9653]
